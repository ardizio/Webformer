{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import bs4\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python has a recursion limit of 3K.\n",
    "# We raise it up to 10K\n",
    "sys.setrecursionlimit(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_RAW_DATA_PATH = './data/endata/'\n",
    "DIR_CLEAN_DATA = './data/endata_new_clean/'\n",
    "JSON_TRAIN_CORPUS = './data/wiki_html_all.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATH: ./data/endata/auto/auto-aol(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/autoauto-aol/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/book/book-abebooks(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/bookbook-abebooks/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/camera/camera-amazon(1767)/0000.htm\n",
      "FILE: ./data/endata_new_clean/cameracamera-amazon/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/job/job-careerbuilder(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/jobjob-careerbuilder/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/movie/movie-allmovie(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/moviemovie-allmovie/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/nbaplayer/nbaplayer-espn(434)/0000.htm\n",
      "FILE: ./data/endata_new_clean/nbaplayernbaplayer-espn/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/restaurant/restaurant-fodors(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/restaurantrestaurant-fodors/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/university/university-collegeboard(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/universityuniversity-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/auto/auto-aol(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/auto-aol-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/auto/auto-autobytel(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/auto-autobytel-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/auto/auto-automotive(1999)/0000.htm\n",
      "FILE: ./data/endata_new_clean/auto-automotive-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/auto/auto-autoweb(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/auto-autoweb-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/auto/auto-carquotes(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/auto-carquotes-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/auto/auto-cars(657)/0000.htm\n",
      "FILE: ./data/endata_new_clean/auto-cars-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/auto/auto-kbb(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/auto-kbb-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/auto/auto-motortrend(1267)/0000.htm\n",
      "FILE: ./data/endata_new_clean/auto-motortrend-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/auto/auto-msn(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/auto-msn-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/auto/auto-yahoo(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/auto-yahoo-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/book/book-abebooks(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/book-abebooks-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/book/book-amazon(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/book-amazon-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/book/book-barnesandnoble(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/book-barnesandnoble-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/book/book-bookdepository(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/book-bookdepository-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/book/book-booksamillion(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/book-booksamillion-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/book/book-borders(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/book-borders-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/book/book-buy(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/book-buy-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/book/book-christianbook(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/book-christianbook-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/book/book-deepdiscount(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/book-deepdiscount-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/book/book-waterstones(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/book-waterstones-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/camera/camera-amazon(1767)/0000.htm\n",
      "FILE: ./data/endata_new_clean/camera-amazon-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/camera/camera-beachaudio(247)/0000.htm\n",
      "FILE: ./data/endata_new_clean/camera-beachaudio-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/camera/camera-buy(500)/0000.htm\n",
      "FILE: ./data/endata_new_clean/camera-buy-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/camera/camera-compsource(430)/0000.htm\n",
      "FILE: ./data/endata_new_clean/camera-compsource-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/camera/camera-ecost(923)/0000.htm\n",
      "FILE: ./data/endata_new_clean/camera-ecost-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/camera/camera-jr(367)/0000.htm\n",
      "FILE: ./data/endata_new_clean/camera-jr-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/camera/camera-newegg(220)/0000.htm\n",
      "FILE: ./data/endata_new_clean/camera-newegg-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/camera/camera-onsale(261)/0000.htm\n",
      "FILE: ./data/endata_new_clean/camera-onsale-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/camera/camera-pcnation(234)/0000.htm\n",
      "FILE: ./data/endata_new_clean/camera-pcnation-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/camera/camera-thenerds(309)/0000.htm\n",
      "FILE: ./data/endata_new_clean/camera-thenerds-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/job/job-careerbuilder(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/job-careerbuilder-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/job/job-dice(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/job-dice-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/job/job-hotjobs(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/job-hotjobs-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/job/job-job(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/job-job-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/job/job-jobcircle(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/job-jobcircle-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/job/job-jobtarget(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/job-jobtarget-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/job/job-monster(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/job-monster-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/job/job-nettemps(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/job-nettemps-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/job/job-rightitjobs(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/job-rightitjobs-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/job/job-techcentric(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/job-techcentric-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/movie/movie-allmovie(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/movie-allmovie-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/movie/movie-amctv(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/movie-amctv-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/movie/movie-boxofficemojo(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/movie-boxofficemojo-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/movie/movie-hollywood(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/movie-hollywood-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/movie/movie-iheartmovies(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/movie-iheartmovies-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/movie/movie-imdb(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/movie-imdb-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/movie/movie-metacritic(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/movie-metacritic-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/movie/movie-msn(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/movie-msn-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/movie/movie-rottentomatoes(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/movie-rottentomatoes-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/movie/movie-yahoo(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/movie-yahoo-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/nbaplayer/nbaplayer-espn(434)/0000.htm\n",
      "FILE: ./data/endata_new_clean/nbaplayer-espn-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/nbaplayer/nbaplayer-fanhouse(446)/0000.htm\n",
      "FILE: ./data/endata_new_clean/nbaplayer-fanhouse-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/nbaplayer/nbaplayer-foxsports(425)/0000.htm\n",
      "FILE: ./data/endata_new_clean/nbaplayer-foxsports-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/nbaplayer/nbaplayer-msnca(434)/0000.htm\n",
      "FILE: ./data/endata_new_clean/nbaplayer-msnca-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/nbaplayer/nbaplayer-nba(434)/0000.htm\n",
      "FILE: ./data/endata_new_clean/nbaplayer-nba-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/nbaplayer/nbaplayer-si(515)/0000.htm\n",
      "FILE: ./data/endata_new_clean/nbaplayer-si-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/nbaplayer/nbaplayer-slam(423)/0000.htm\n",
      "FILE: ./data/endata_new_clean/nbaplayer-slam-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/nbaplayer/nbaplayer-usatoday(436)/0000.htm\n",
      "FILE: ./data/endata_new_clean/nbaplayer-usatoday-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/nbaplayer/nbaplayer-wiki(420)/0000.htm\n",
      "FILE: ./data/endata_new_clean/nbaplayer-wiki-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/nbaplayer/nbaplayer-yahoo(438)/0000.htm\n",
      "FILE: ./data/endata_new_clean/nbaplayer-yahoo-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/restaurant/restaurant-fodors(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/restaurant-fodors-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/restaurant/restaurant-frommers(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/restaurant-frommers-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/restaurant/restaurant-gayot(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/restaurant-gayot-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/restaurant/restaurant-opentable(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/restaurant-opentable-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/restaurant/restaurant-pickarestaurant(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/restaurant-pickarestaurant-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/restaurant/restaurant-restaurantica(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/restaurant-restaurantica-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/restaurant/restaurant-tripadvisor(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/restaurant-tripadvisor-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/restaurant/restaurant-urbanspoon(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/restaurant-urbanspoon-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/restaurant/restaurant-usdiners(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/restaurant-usdiners-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/restaurant/restaurant-zagat(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/restaurant-zagat-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/university/university-collegeboard(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/university-collegeboard-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/university/university-collegenavigator(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/university-collegenavigator-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/university/university-collegeprowler(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/university-collegeprowler-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/university/university-collegetoolkit(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/university-collegetoolkit-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/university/university-ecampustours(1063)/0000.htm\n",
      "FILE: ./data/endata_new_clean/university-ecampustours-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/university/university-embark(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/university-embark-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/university/university-matchcollege(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/university-matchcollege-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/university/university-princetonreview(615)/0000.htm\n",
      "FILE: ./data/endata_new_clean/university-princetonreview-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/university/university-studentaid(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/university-studentaid-university-collegeboard/clean_0000.htm\n",
      "\n",
      "PATH: ./data/endata/university/university-usnews(1027)/0000.htm\n",
      "FILE: ./data/endata_new_clean/university-usnews-university-collegeboard/clean_0000.htm\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Open Train Corpus file\n",
    "with open(JSON_TRAIN_CORPUS,'w')as g:\n",
    "    for root, dirs, files in os.walk(DIR_RAW_DATA_PATH):\n",
    "\n",
    "        iter_i = 0\n",
    "        # Iter ZERO\n",
    "        for dir in dirs:\n",
    "            #print(\"DIR: \"+dir)\n",
    "            sub_DIR_RAW_DATA_PATH = os.path.join(root,dir)\n",
    "            # Get folder\n",
    "            iter_j=0\n",
    "            # Iter ONE\n",
    "            for sub_root,sub_dirs,sub_files in os.walk(sub_DIR_RAW_DATA_PATH):\n",
    "                if iter_j > 0:\n",
    "                    extracted_name = sub_root.split(\"\\\\\")[1].split(\"(\")[0]\n",
    "                    #print(\"FOLDER: \"+extracted_name)\n",
    "                iter_j += 1\n",
    "                # Get file\n",
    "                iter_k = 0\n",
    "                # Iter TWO\n",
    "                for item_parsed in sub_files:\n",
    "                    path = os.path.join(sub_root,item_parsed)\n",
    "                    path = re.sub(r'\\\\', '/', path)\n",
    "                    clean_path = os.path.join(DIR_CLEAN_DATA,dir+extracted_name+\"/clean_\"+item_parsed)\n",
    "                    clean_path = re.sub(r'\\((\\d+)\\)', '-', clean_path)\n",
    "                    print(f\"PATH: {path}\")\n",
    "                    print(f\"FILE: {clean_path}\")\n",
    "                    print(\"\")\n",
    "                    if iter_k == 0:\n",
    "                        break\n",
    "                    iter_k += 1\n",
    "\n",
    "                if iter_j == 2:\n",
    "                    break\n",
    "            iter_i =+1 \n",
    "\n",
    "                \n",
    "\n",
    "        \"\"\" \n",
    "        for dir in tqdm(dirs):\n",
    "            sub_DIR_RAW_DATA_PATH = os.path.join(root,dir)\n",
    "            for sub_root,sub_dirs,sub_files in os.walk(sub_DIR_RAW_DATA_PATH):\n",
    "                # foeach file in dir\n",
    "                for item_parsed in tqdm(sub_files):\n",
    "                    #index = item_parsed.split('.')[0]\n",
    "                    path = os.path.join(sub_root,item_parsed)\n",
    "                    clean_path = DIR_CLEAN_DATA+'_clean.html'\n",
    "                    cleaner = HTMLCleaner(path,clean_path)\n",
    "                    cleaner.clean_html()\n",
    "                    #cleaner.clean_tree()\n",
    "                    storer = HTMLStorer(input_file=None,html=cleaner.line)\n",
    "                    g.write(json.dumps(storer.data,ensure_ascii=False)+'\\n') \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTMLCleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HTMLCleaner(object):\n",
    "    def __init__(self, input_file, output_file):\n",
    "        self.input_file = input_file\n",
    "        self.output_file = output_file\n",
    "        self.line = \"\"\n",
    "        \n",
    "    # CLEAN HTML String --------------------------------------- start\n",
    "    def clean_tag(self, x):\n",
    "        '''Split the matched tag into components\n",
    "        Check if the first component starts with '<'\n",
    "        If it does, reconstruct the tag with the first and second components\n",
    "        If not, reconstruct the tag with the first component\n",
    "        Return the cleaned tag'''\n",
    "\n",
    "        x = x.group().split()\n",
    "        if x[0] == '<': \n",
    "            y = x[0] + x[1].strip('>') + '>'\n",
    "        else:\n",
    "            y = x[0].strip('>') + '>'\n",
    "        return y\n",
    "    \n",
    "\n",
    "    def clean_html(self):\n",
    "        '''Preprocess an HTML string and cleans it\n",
    "\n",
    "        Define regular expressions for different HTML elements\n",
    "        Compile regular expressions into patterns\n",
    "        Initialize lists to store different elements\n",
    "\n",
    "        Open file\n",
    "        - Extract annotations\n",
    "        - Extract close tags\n",
    "        - Extract styles\n",
    "        - Extract all tags\n",
    "\n",
    "        Store the cleaned line'''\n",
    "\n",
    "        style_regex = \"(?:<style.*?>(?:.|[\\r\\n])*?</style>|<script.*?>(?:.|[\\r\\n])*?</script>)\"\n",
    "        all_tag_regex = \"(?:<(?:!|/?[a-zA-Z]+).*?/?>)\"\n",
    "        close_tag_regex = '(<(?:!|/?[a-zA-Z]+)[^>]*?/>){1}?'\n",
    "        annotation_regex = '(?:<!--(?:.|[\\r\\n])*?-->)'\n",
    "        close_tag_pattern = re.compile(close_tag_regex)\n",
    "        annotation_pattern = re.compile(annotation_regex)\n",
    "        all_tag_pattern = re.compile(all_tag_regex)\n",
    "        style_pattern = re.compile(style_regex)\n",
    "        tags = []\n",
    "        annotation = []\n",
    "        close_tag = []\n",
    "        all_tag = []\n",
    "        style = []\n",
    "        with open(self.input_file, 'r') as f:\n",
    "            line = f.read()\n",
    "            annotation.extend(annotation_pattern.findall(line))\n",
    "            #print(annotation)\n",
    "            line = re.sub(annotation_regex, '', line)\n",
    "            close_tag.extend(close_tag_pattern.findall(line))\n",
    "            #print(close_tag)\n",
    "            line = re.sub(close_tag_regex, '', line)\n",
    "            style.extend(style_pattern.findall(line))\n",
    "            #print(style)\n",
    "            line = re.sub(style_regex, '', line)\n",
    "            all_tag.extend(all_tag_pattern.findall(line))\n",
    "            #print(all_tag)\n",
    "            line = re.sub(all_tag_regex, self.clean_tag, line)  # Note: self.clean_tag is missing\n",
    "        #print(line)\n",
    "        self.line = line\n",
    "    # CLEAN HTML String --------------------------------------- end\n",
    "\n",
    "\n",
    "\n",
    "    # BUILD THREE --------------------------------------- start\n",
    "    def if_text(self, node):\n",
    "        '''\n",
    "        Check if the text content of the HTML or XML node is empty\n",
    "        Return 0 if the text is empty (considered \"falsy\")\n",
    "        Return 1 if the text is not empty (considered \"truthy\")'''\n",
    "        if node.get_text('|', strip=True) == \"\":\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    \n",
    "    def get_tree(self,root):\n",
    "        '''\n",
    "        Recursively processes the HTML/XML tree rooted at 'root', removing elements with empty text content.\n",
    "        Args: root (bs4.element.Tag): The root of the HTML/XML tree.\n",
    "        Returns: bs4.element.Tag: The modified root after removing elements with empty text content.'''\n",
    "        delete_list = []\n",
    "        # Iterate through the children of the root\n",
    "        for child in root.children:\n",
    "            # Check if the child is a bs4.element.Tag\n",
    "            if type(child) == bs4.element.Tag:\n",
    "                # Check if the text content of the tag is empty\n",
    "                if not self.if_text(child):\n",
    "                    delete_list.append(child)\n",
    "                else:\n",
    "                    # Recursively process child if it's an HTML tag\n",
    "                    child = self.get_tree(child)\n",
    "            elif type(child) == bs4.element.NavigableString:\n",
    "                # Check if the NavigableString (text) is empty\n",
    "                if str(child).strip() == \"\":\n",
    "                    delete_list.append(child)\n",
    "        # Remove items in the delete_list from the tree\n",
    "        for item in delete_list:\n",
    "            item.extract()\n",
    "        # Return the modified root\n",
    "        return root\n",
    "    \n",
    "    def merge_tree(self,root,k):\n",
    "        '''\n",
    "        Merges children of an HTML/XML tree node if the number of children is less than or equal to 'k'.\n",
    "        Args: root (bs4.element.Tag): The root of the HTML/XML tree node.\n",
    "              k (int): The threshold for the number of children to trigger merging.\n",
    "        Returns:  bs4.element.Tag: The modified root after merging children if necessary.'''\n",
    "        i = 0\n",
    "        # Iterate through the children of the root\n",
    "        while(i<len(root.contents)):\n",
    "            # Check if the child is an HTML tag\n",
    "            if(type(root.contents[i])==bs4.element.Tag):\n",
    "                # Check if the number of children is less than or equal to 'k'\n",
    "                if len(root.contents[i].contents)<=k:\n",
    "                    tmp = root.contents[i]\n",
    "                    j = i\n",
    "                    # Remove the current child from the tree\n",
    "                    del root.contents[i]\n",
    "                    # Insert the contents of the removed child at the current position\n",
    "                    for item in tmp.contents:\n",
    "                        root.contents.insert(j,item)\n",
    "                        j+=1\n",
    "                else:\n",
    "                    # Recursively merge children if the number of children exceeds 'k'\n",
    "                    root.contents[i] = self.merge_tree(root.contents[i],k)\n",
    "                    i+=1\n",
    "            else:\n",
    "                # Skip non-tag elements\n",
    "                i+=1\n",
    "        # Return the modified root after merging children\n",
    "        return root\n",
    "                    \n",
    "    def clean_tree(self):\n",
    "        root = BeautifulSoup(self.line,'html.parser').html\n",
    "        root = self.get_tree(root)\n",
    "        root = self.merge_tree(root,2)\n",
    "        self.root = root\n",
    "    # BUILD THREE --------------------------------------- end\n",
    "\n",
    "\n",
    "    def store(self):\n",
    "        with open(self.output_file,'w+')as f:\n",
    "            f.write(str(self.root))\n",
    "\n",
    "m_path = \"./data/endata/auto/auto-aol(2000)/0000.htm\"\n",
    "m_clean_path = \"./data/endata_new_clean/autoauto-aol/clean_0000.htm\"\n",
    "cleaner = HTMLCleaner(m_path, m_clean_path)\n",
    "cleaner.clean_html()\n",
    "cleaner.clean_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTMLStorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HTMLStorer(object):\n",
    "    def __init__(self,input_file=None,html=None):\n",
    "        '''\n",
    "        Accepts as input_file or html, after: cleaner.clean_html() AND cleaner.clean_tree()\n",
    "        '''\n",
    "        # Check if neither input_file nor html is provided\n",
    "        if not input_file and not html:\n",
    "            #print(\"CASE-ERROR\")\n",
    "            raise ValueError(\"lack of input file or html\")\n",
    "        # If input_file is provided, read its contents and assign to html_text\n",
    "        if input_file:\n",
    "            #print(\"CASE-1\")\n",
    "            with open(input_file, 'r') as f:\n",
    "                self.html_text = f.read()\n",
    "        # If html is provided, assign it to html_text\n",
    "        else:\n",
    "            #print(\"CASE-2\")\n",
    "            self.html_text = html\n",
    "        self.soup = BeautifulSoup(self.html_text, 'html.parser')  # Create BeautifulSoup object using html_text and html.parser\n",
    "        self.root = self.soup      # Set the root of the HTML tree to the BeautifulSoup object\n",
    "        self.root.depth = 0        # Set the depth of the root to 0\n",
    "        self.depth = {}            # Initialize an empty dictionary for depth information\n",
    "        self.idx = 0               # Initialize an index variable\n",
    "        self.data = []             # Initialize an empty list for storing data\n",
    "        self.root = self.add_text_node(self.root)  # Add text nodes to the HTML tree and update the root\n",
    "        self.get_index(self.root)  # Get the index of each node in the HTML tree\n",
    "        self.get_data(self.root)   # Get data from each node in the HTML tree\n",
    "\n",
    "    \n",
    "    def add_text_node(self, root):\n",
    "        '''\n",
    "        Recursively traverse the HTML tree and wrap NavigableString nodes in a 'textnode' tag.\n",
    "        Parameters: root (bs4.element.Tag): The current node in the HTML tree.\n",
    "        Returns: bs4.element.Tag: The modified HTML tree with text nodes wrapped.\n",
    "        \n",
    "        try:\n",
    "            if: If the child is a Tag and not named 'textnode', recursively call add_text_node\n",
    "            elif: If the child is a NavigableString, wrap it in a 'textnode' tag\n",
    "        except:\n",
    "            Handle exceptions (consider providing a more specific exception type)\n",
    "        '''\n",
    "        for child in root.children:\n",
    "            try:\n",
    "                if isinstance(child, bs4.element.Tag) and child.name != \"textnode\":\n",
    "                    #print('atn-RECURSION')\n",
    "                    child = self.add_text_node(child)\n",
    "                elif isinstance(child, bs4.element.NavigableString):\n",
    "                    #print('atn-DONE')\n",
    "                    child.wrap(self.soup.new_tag(\"textnode\"))\n",
    "            except Exception as e:\n",
    "                print(f\"Exception while processing node: {root}. Exception: {e}\")\n",
    "        return root\n",
    "\n",
    "\n",
    "    def get_index(self, node):\n",
    "        '''\n",
    "        Recursively traverse the HTML tree and assign index and depth values to each Tag.\n",
    "        Parameters: node (bs4.element.Tag): The current node in the HTML tree.\n",
    "        Returns: None'''\n",
    "        for child in node.children:\n",
    "            if type(child) != bs4.element.Tag:\n",
    "                continue\n",
    "            # Assign index and depth values to the current child Tag\n",
    "            child.idx = self.idx\n",
    "            child.depth = node.depth + 1\n",
    "            # Update the depth dictionary with the count of tags at the current depth\n",
    "            if child.depth in self.depth:\n",
    "                self.depth[child.depth] += 1\n",
    "            else:\n",
    "                self.depth[child.depth] = 1\n",
    "            # Increment the index for the next Tag\n",
    "            self.idx += 1\n",
    "            # Recursively call get_index for the child Tag\n",
    "            self.get_index(child)\n",
    "    \n",
    "    def get_tag_text(self, node):\n",
    "        '''\n",
    "        Extract and concatenate text content from NavigableString children of the given HTML node.\n",
    "        Parameters:  node (bs4.element.Tag): The HTML node from which to extract text content.\n",
    "        Returns: str: Concatenated and formatted text content. '''\n",
    "        line = \"\"  # Initialize an empty string to store the concatenated text\n",
    "        for child in node.children:\n",
    "            if isinstance(child, bs4.element.NavigableString):\n",
    "                x = str(child).strip().replace('\\n', ' ')  # Convert NavigableString to string, strip, replace newlines, and concatenate\n",
    "                if x != \"\":\n",
    "                    line = line + '\\t' + x.strip()\n",
    "\n",
    "        return line.strip()  # Return the concatenated and formatted text content\n",
    "\n",
    "    def get_data(self,node):\n",
    "        for child_node in node.children:\n",
    "            if type(child_node)!= bs4.element.Tag:\n",
    "                continue\n",
    "            else:\n",
    "                name = child_node.name\n",
    "                node_id = child_node.idx\n",
    "                node_text = self.get_tag_text(child_node)\n",
    "                node_child_idx = []\n",
    "                for item in child_node.children:\n",
    "                    if type(item) == bs4.element.Tag:\n",
    "                        node_child_idx.append(item.idx)\n",
    "                line = {\"name\":name,\"id\":node_id,\"text\":node_text,\"children\":node_child_idx}\n",
    "                self.data.append(line)\n",
    "                self.get_data(child_node)\n",
    "    \n",
    "\n",
    "    def get_data(self, node):\n",
    "        '''\n",
    "        Recursively traverse the HTML tree and gather information about each Tag.\n",
    "        Parameters: node (bs4.element.Tag): The current node in the HTML tree.\n",
    "        Returns: None\n",
    "        '''\n",
    "        for child_node in node.children:\n",
    "            if not isinstance(child_node, bs4.element.Tag):\n",
    "                continue # Skip non-Tag elements\n",
    "            else:\n",
    "                # Extract information about the current Tag\n",
    "                name = child_node.name\n",
    "                node_id = child_node.idx\n",
    "                node_text = self.get_tag_text(child_node)\n",
    "                # Gather indices of children that are Tags\n",
    "                node_child_idx = []\n",
    "                for item in child_node.children:\n",
    "                    if type(item) == bs4.element.Tag:\n",
    "                        node_child_idx.append(item.idx)\n",
    "                # Create a dictionary representing the current Tag\n",
    "                line = {\"name\": name, \"id\": node_id, \"text\": node_text, \"children\": node_child_idx}\n",
    "                # Append the dictionary to the data list\n",
    "                self.data.append(line)\n",
    "                # Recursively call get_data for the child Tag\n",
    "                self.get_data(child_node)\n",
    "\n",
    "\n",
    "    def store(self,output_file):\n",
    "        with open(output_file,'a')as g:\n",
    "            g.write(str(self.idx)+'\\t'+json.dumps(self.depth,ensure_ascii=False)+'\\n')\n",
    "\n",
    "\n",
    "\n",
    "storer = HTMLStorer(input_file=None,html=cleaner.line)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
