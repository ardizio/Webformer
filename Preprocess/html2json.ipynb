{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import bs4\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python has a recursion limit of 3K.\n",
    "# We raise it up to 10K\n",
    "sys.setrecursionlimit(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTMLCleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HTMLCleaner(object):\n",
    "    def __init__(self, input_file, output_file):\n",
    "        self.input_file = input_file\n",
    "        self.output_file = output_file\n",
    "        self.line = \"\"\n",
    "        \n",
    "    # CLEAN HTML String --------------------------------------- start\n",
    "    def clean_tag(self, x):\n",
    "        '''Split the matched tag into components\n",
    "        Check if the first component starts with '<'\n",
    "        If it does, reconstruct the tag with the first and second components\n",
    "        If not, reconstruct the tag with the first component\n",
    "        Return the cleaned tag'''\n",
    "\n",
    "        x = x.group().split()\n",
    "        if x[0] == '<': \n",
    "            y = x[0] + x[1].strip('>') + '>'\n",
    "        else:\n",
    "            y = x[0].strip('>') + '>'\n",
    "        return y\n",
    "    \n",
    "\n",
    "    def clean_html(self):\n",
    "        '''Preprocess an HTML string and cleans it\n",
    "\n",
    "        Define regular expressions for different HTML elements\n",
    "        Compile regular expressions into patterns\n",
    "        Initialize lists to store different elements\n",
    "\n",
    "        Open file\n",
    "        - Extract annotations\n",
    "        - Extract close tags\n",
    "        - Extract styles\n",
    "        - Extract all tags\n",
    "\n",
    "        Store the cleaned line'''\n",
    "\n",
    "        style_regex = \"(?:<style.*?>(?:.|[\\r\\n])*?</style>|<script.*?>(?:.|[\\r\\n])*?</script>)\"\n",
    "        all_tag_regex = \"(?:<(?:!|/?[a-zA-Z]+).*?/?>)\"\n",
    "        close_tag_regex = '(<(?:!|/?[a-zA-Z]+)[^>]*?/>){1}?'\n",
    "        annotation_regex = '(?:<!--(?:.|[\\r\\n])*?-->)'\n",
    "        close_tag_pattern = re.compile(close_tag_regex)\n",
    "        annotation_pattern = re.compile(annotation_regex)\n",
    "        all_tag_pattern = re.compile(all_tag_regex)\n",
    "        style_pattern = re.compile(style_regex)\n",
    "        tags = []\n",
    "        annotation = []\n",
    "        close_tag = []\n",
    "        all_tag = []\n",
    "        style = []\n",
    "        with open(self.input_file, 'r') as f:\n",
    "            line = f.read()\n",
    "            annotation.extend(annotation_pattern.findall(line))\n",
    "            #print(annotation)\n",
    "            line = re.sub(annotation_regex, '', line)\n",
    "            close_tag.extend(close_tag_pattern.findall(line))\n",
    "            #print(close_tag)\n",
    "            line = re.sub(close_tag_regex, '', line)\n",
    "            style.extend(style_pattern.findall(line))\n",
    "            #print(style)\n",
    "            line = re.sub(style_regex, '', line)\n",
    "            all_tag.extend(all_tag_pattern.findall(line))\n",
    "            #print(all_tag)\n",
    "            line = re.sub(all_tag_regex, self.clean_tag, line)  # Note: self.clean_tag is missing\n",
    "            line = re.sub(r'ï»¿', '', line)\n",
    "        #print(line)\n",
    "        self.line = line\n",
    "    # CLEAN HTML String --------------------------------------- end\n",
    "\n",
    "\n",
    "\n",
    "    # BUILD THREE --------------------------------------- start\n",
    "    def if_text(self, node):\n",
    "        '''\n",
    "        Check if the text content of the HTML or XML node is empty\n",
    "        Return 0 if the text is empty (considered \"falsy\")\n",
    "        Return 1 if the text is not empty (considered \"truthy\")'''\n",
    "        if node.get_text('|', strip=True) == \"\":\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    \n",
    "    def get_tree(self,root):\n",
    "        '''\n",
    "        Recursively processes the HTML/XML tree rooted at 'root', removing elements with empty text content.\n",
    "        Args: root (bs4.element.Tag): The root of the HTML/XML tree.\n",
    "        Returns: bs4.element.Tag: The modified root after removing elements with empty text content.'''\n",
    "        delete_list = []\n",
    "        # Iterate through the children of the root\n",
    "        for child in root.children:\n",
    "            # Check if the child is a bs4.element.Tag\n",
    "            if type(child) == bs4.element.Tag:\n",
    "                # Check if the text content of the tag is empty\n",
    "                if not self.if_text(child):\n",
    "                    delete_list.append(child)\n",
    "                else:\n",
    "                    # Recursively process child if it's an HTML tag\n",
    "                    child = self.get_tree(child)\n",
    "            elif type(child) == bs4.element.NavigableString:\n",
    "                # Check if the NavigableString (text) is empty\n",
    "                if str(child).strip() == \"\":\n",
    "                    delete_list.append(child)\n",
    "        # Remove items in the delete_list from the tree\n",
    "        for item in delete_list:\n",
    "            item.extract()\n",
    "        # Return the modified root\n",
    "        return root\n",
    "    \n",
    "    def merge_tree(self,root,k):\n",
    "        '''\n",
    "        Merges children of an HTML/XML tree node if the number of children is less than or equal to 'k'.\n",
    "        Args: root (bs4.element.Tag): The root of the HTML/XML tree node.\n",
    "              k (int): The threshold for the number of children to trigger merging.\n",
    "        Returns:  bs4.element.Tag: The modified root after merging children if necessary.'''\n",
    "        i = 0\n",
    "        # Iterate through the children of the root\n",
    "        while(i<len(root.contents)):\n",
    "            # Check if the child is an HTML tag\n",
    "            if(type(root.contents[i])==bs4.element.Tag):\n",
    "                # Check if the number of children is less than or equal to 'k'\n",
    "                if len(root.contents[i].contents)<=k:\n",
    "                    tmp = root.contents[i]\n",
    "                    j = i\n",
    "                    # Remove the current child from the tree\n",
    "                    del root.contents[i]\n",
    "                    # Insert the contents of the removed child at the current position\n",
    "                    for item in tmp.contents:\n",
    "                        root.contents.insert(j,item)\n",
    "                        j+=1\n",
    "                else:\n",
    "                    # Recursively merge children if the number of children exceeds 'k'\n",
    "                    root.contents[i] = self.merge_tree(root.contents[i],k)\n",
    "                    i+=1\n",
    "            else:\n",
    "                # Skip non-tag elements\n",
    "                i+=1\n",
    "        # Return the modified root after merging children\n",
    "        return root\n",
    "                    \n",
    "    def clean_tree(self):\n",
    "        root = BeautifulSoup(self.line,'html.parser').html\n",
    "        root = self.get_tree(root)\n",
    "        root = self.merge_tree(root,2)\n",
    "        self.root = root\n",
    "    # BUILD THREE --------------------------------------- end\n",
    "\n",
    "\n",
    "    def store(self):\n",
    "        with open(self.output_file,'w+')as f:\n",
    "            f.write(str(self.root))\n",
    "\n",
    "m_path = \"./data/endata/auto/auto-aol(2000)/0000.htm\"\n",
    "m_clean_path = \"./data/endata_new_clean/autoauto-aol/clean_0000.htm\"\n",
    "cleaner = HTMLCleaner(m_path, m_clean_path)\n",
    "cleaner.clean_html()\n",
    "cleaner.clean_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTMLStorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HTMLStorer(object):\n",
    "    def __init__(self,input_file=None,html=None):\n",
    "        '''\n",
    "        Accepts as input_file or html, after: cleaner.clean_html() AND cleaner.clean_tree()\n",
    "        '''\n",
    "        # Check if neither input_file nor html is provided\n",
    "        if not input_file and not html:\n",
    "            #print(\"CASE-ERROR\")\n",
    "            raise ValueError(\"lack of input file or html\")\n",
    "        # If input_file is provided, read its contents and assign to html_text\n",
    "        if input_file:\n",
    "            #print(\"CASE-1\")\n",
    "            with open(input_file, 'r') as f:\n",
    "                self.html_text = f.read()\n",
    "        # If html is provided, assign it to html_text\n",
    "        else:\n",
    "            #print(\"CASE-2\")\n",
    "            self.html_text = html\n",
    "        self.soup = BeautifulSoup(self.html_text, 'html.parser')  # Create BeautifulSoup object using html_text and html.parser\n",
    "        self.root = self.soup      # Set the root of the HTML tree to the BeautifulSoup object\n",
    "        self.root.depth = 0        # Set the depth of the root to 0\n",
    "        self.depth = {}            # Initialize an empty dictionary for depth information\n",
    "        self.idx = 0               # Initialize an index variable\n",
    "        self.data = []             # Initialize an empty list for storing data\n",
    "        self.root = self.add_text_node(self.root)  # Add text nodes to the HTML tree and update the root\n",
    "        self.get_index(self.root)  # Get the index of each node in the HTML tree\n",
    "        self.get_data(self.root)   # Get data from each node in the HTML tree\n",
    "\n",
    "    \n",
    "    def add_text_node(self, root):\n",
    "        '''\n",
    "        Recursively traverse the HTML tree and wrap NavigableString nodes in a 'textnode' tag.\n",
    "        Parameters: root (bs4.element.Tag): The current node in the HTML tree.\n",
    "        Returns: bs4.element.Tag: The modified HTML tree with text nodes wrapped.\n",
    "        \n",
    "        try:\n",
    "            if: If the child is a Tag and not named 'textnode', recursively call add_text_node\n",
    "            elif: If the child is a NavigableString, wrap it in a 'textnode' tag\n",
    "        except:\n",
    "            Handle exceptions (consider providing a more specific exception type)\n",
    "        '''\n",
    "        for child in root.children:\n",
    "            try:\n",
    "                if isinstance(child, bs4.element.Tag) and child.name != \"textnode\":\n",
    "                    #print('atn-RECURSION')\n",
    "                    child = self.add_text_node(child)\n",
    "                elif isinstance(child, bs4.element.NavigableString):\n",
    "                    #print('atn-DONE')\n",
    "                    child.wrap(self.soup.new_tag(\"textnode\"))\n",
    "            except Exception as e:\n",
    "                print(f\"Exception while processing node: {root}. Exception: {e}\")\n",
    "        return root\n",
    "\n",
    "\n",
    "    def get_index(self, node):\n",
    "        '''\n",
    "        Recursively traverse the HTML tree and assign index and depth values to each Tag.\n",
    "        Parameters: node (bs4.element.Tag): The current node in the HTML tree.\n",
    "        Returns: None'''\n",
    "        for child in node.children:\n",
    "            if type(child) != bs4.element.Tag:\n",
    "                continue\n",
    "            # Assign index and depth values to the current child Tag\n",
    "            child.idx = self.idx\n",
    "            child.depth = node.depth + 1\n",
    "            # Update the depth dictionary with the count of tags at the current depth\n",
    "            if child.depth in self.depth:\n",
    "                self.depth[child.depth] += 1\n",
    "            else:\n",
    "                self.depth[child.depth] = 1\n",
    "            # Increment the index for the next Tag\n",
    "            self.idx += 1\n",
    "            # Recursively call get_index for the child Tag\n",
    "            self.get_index(child)\n",
    "    \n",
    "    def get_tag_text(self, node):\n",
    "        '''\n",
    "        Extract and concatenate text content from NavigableString children of the given HTML node.\n",
    "        Parameters:  node (bs4.element.Tag): The HTML node from which to extract text content.\n",
    "        Returns: str: Concatenated and formatted text content. '''\n",
    "        line = \"\"  # Initialize an empty string to store the concatenated text\n",
    "        for child in node.children:\n",
    "            if isinstance(child, bs4.element.NavigableString):\n",
    "                x = str(child).strip().replace('\\n', ' ')  # Convert NavigableString to string, strip, replace newlines, and concatenate\n",
    "                if x != \"\":\n",
    "                    line = line + '\\t' + x.strip()\n",
    "\n",
    "        return line.strip()  # Return the concatenated and formatted text content\n",
    "    \n",
    "    def get_data(self, node):\n",
    "        '''\n",
    "        Recursively traverse the HTML tree and gather information about each Tag.\n",
    "        Parameters: node (bs4.element.Tag): The current node in the HTML tree.\n",
    "        Returns: None\n",
    "        '''\n",
    "        for child_node in node.children:\n",
    "            if not isinstance(child_node, bs4.element.Tag):\n",
    "                continue # Skip non-Tag elements\n",
    "            else:\n",
    "                # Extract information about the current Tag\n",
    "                name = child_node.name\n",
    "                node_id = child_node.idx\n",
    "                node_text = self.get_tag_text(child_node)\n",
    "                # Gather indices of children that are Tags\n",
    "                node_child_idx = []\n",
    "                for item in child_node.children:\n",
    "                    if type(item) == bs4.element.Tag:\n",
    "                        node_child_idx.append(item.idx)\n",
    "                # Create a dictionary representing the current Tag\n",
    "                line = {\"name\": name, \"id\": node_id, \"text\": node_text, \"children\": node_child_idx}\n",
    "                # Append the dictionary to the data list\n",
    "                self.data.append(line)\n",
    "                # Recursively call get_data for the child Tag\n",
    "                self.get_data(child_node)\n",
    "\n",
    "    def store(self,output_file):\n",
    "        with open(output_file,'a')as g:\n",
    "            g.write(str(self.idx)+'\\t'+json.dumps(self.depth,ensure_ascii=False)+'\\n')\n",
    "\n",
    "\n",
    "\n",
    "storer = HTMLStorer(input_file=None,html=cleaner.line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_RAW_DATA_PATH = './data/endata/'\n",
    "DIR_CLEAN_DATA = './data/endata_new_clean/'\n",
    "JSON_TRAIN_CORPUS = './data/wiki_html_all.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - 2 - 0\n",
      "PATH: ./data/endata/auto/auto-aol(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/autoauto-aol/clean_0000.htm\n",
      "\n",
      "1 - 2 - 0\n",
      "PATH: ./data/endata/book/book-abebooks(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/bookbook-abebooks/clean_0000.htm\n",
      "\n",
      "1 - 2 - 0\n",
      "PATH: ./data/endata/camera/camera-amazon(1767)/0000.htm\n",
      "FILE: ./data/endata_new_clean/cameracamera-amazon/clean_0000.htm\n",
      "\n",
      "1 - 2 - 0\n",
      "PATH: ./data/endata/job/job-careerbuilder(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/jobjob-careerbuilder/clean_0000.htm\n",
      "\n",
      "1 - 2 - 0\n",
      "PATH: ./data/endata/movie/movie-allmovie(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/moviemovie-allmovie/clean_0000.htm\n",
      "\n",
      "1 - 2 - 0\n",
      "PATH: ./data/endata/nbaplayer/nbaplayer-espn(434)/0000.htm\n",
      "FILE: ./data/endata_new_clean/nbaplayernbaplayer-espn/clean_0000.htm\n",
      "\n",
      "1 - 2 - 0\n",
      "PATH: ./data/endata/restaurant/restaurant-fodors(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/restaurantrestaurant-fodors/clean_0000.htm\n",
      "\n",
      "1 - 2 - 0\n",
      "PATH: ./data/endata/university/university-collegeboard(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/universityuniversity-collegeboard/clean_0000.htm\n",
      "\n",
      "0 - 1 - 0\n",
      "PATH: ./data/endata/auto/auto-aol(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/auto-aol-university-collegeboard/clean_0000.htm\n",
      "\n",
      "1 - 1 - 0\n",
      "PATH: ./data/endata/auto/auto-autobytel(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/auto-autobytel-university-collegeboard/clean_0000.htm\n",
      "\n",
      "1 - 1 - 0\n",
      "PATH: ./data/endata/auto/auto-automotive(1999)/0000.htm\n",
      "FILE: ./data/endata_new_clean/auto-automotive-university-collegeboard/clean_0000.htm\n",
      "\n",
      "1 - 1 - 0\n",
      "PATH: ./data/endata/auto/auto-autoweb(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/auto-autoweb-university-collegeboard/clean_0000.htm\n",
      "\n",
      "1 - 1 - 0\n",
      "PATH: ./data/endata/auto/auto-carquotes(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/auto-carquotes-university-collegeboard/clean_0000.htm\n",
      "\n",
      "1 - 1 - 0\n",
      "PATH: ./data/endata/auto/auto-cars(657)/0000.htm\n",
      "FILE: ./data/endata_new_clean/auto-cars-university-collegeboard/clean_0000.htm\n",
      "\n",
      "1 - 1 - 0\n",
      "PATH: ./data/endata/auto/auto-kbb(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/auto-kbb-university-collegeboard/clean_0000.htm\n",
      "\n",
      "1 - 1 - 0\n",
      "PATH: ./data/endata/auto/auto-motortrend(1267)/0000.htm\n",
      "FILE: ./data/endata_new_clean/auto-motortrend-university-collegeboard/clean_0000.htm\n",
      "\n",
      "1 - 1 - 0\n",
      "PATH: ./data/endata/auto/auto-msn(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/auto-msn-university-collegeboard/clean_0000.htm\n",
      "\n",
      "1 - 1 - 0\n",
      "PATH: ./data/endata/auto/auto-yahoo(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/auto-yahoo-university-collegeboard/clean_0000.htm\n",
      "\n",
      "0 - 1 - 0\n",
      "PATH: ./data/endata/book/book-abebooks(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/book-abebooks-university-collegeboard/clean_0000.htm\n",
      "\n",
      "1 - 1 - 0\n",
      "PATH: ./data/endata/book/book-amazon(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/book-amazon-university-collegeboard/clean_0000.htm\n",
      "\n",
      "1 - 1 - 0\n",
      "PATH: ./data/endata/book/book-barnesandnoble(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/book-barnesandnoble-university-collegeboard/clean_0000.htm\n",
      "\n",
      "1 - 1 - 0\n",
      "PATH: ./data/endata/book/book-bookdepository(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/book-bookdepository-university-collegeboard/clean_0000.htm\n",
      "\n",
      "1 - 1 - 0\n",
      "PATH: ./data/endata/book/book-booksamillion(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/book-booksamillion-university-collegeboard/clean_0000.htm\n",
      "\n",
      "1 - 1 - 0\n",
      "PATH: ./data/endata/book/book-borders(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/book-borders-university-collegeboard/clean_0000.htm\n",
      "\n",
      "1 - 1 - 0\n",
      "PATH: ./data/endata/book/book-buy(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/book-buy-university-collegeboard/clean_0000.htm\n",
      "\n",
      "1 - 1 - 0\n",
      "PATH: ./data/endata/book/book-christianbook(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/book-christianbook-university-collegeboard/clean_0000.htm\n",
      "\n",
      "1 - 1 - 0\n",
      "PATH: ./data/endata/book/book-deepdiscount(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/book-deepdiscount-university-collegeboard/clean_0000.htm\n",
      "\n",
      "1 - 1 - 0\n",
      "PATH: ./data/endata/book/book-waterstones(2000)/0000.htm\n",
      "FILE: ./data/endata_new_clean/book-waterstones-university-collegeboard/clean_0000.htm\n",
      "\n",
      "0 - 1 - 0\n",
      "PATH: ./data/endata/camera/camera-amazon(1767)/0000.htm\n",
      "FILE: ./data/endata_new_clean/camera-amazon-university-collegeboard/clean_0000.htm\n",
      "\n",
      "1 - 1 - 0\n",
      "PATH: ./data/endata/camera/camera-beachaudio(247)/0000.htm\n",
      "FILE: ./data/endata_new_clean/camera-beachaudio-university-collegeboard/clean_0000.htm\n",
      "\n",
      "1 - 1 - 0\n",
      "PATH: ./data/endata/camera/camera-buy(500)/0000.htm\n",
      "FILE: ./data/endata_new_clean/camera-buy-university-collegeboard/clean_0000.htm\n",
      "\n",
      "1 - 1 - 0\n",
      "PATH: ./data/endata/camera/camera-compsource(430)/0000.htm\n",
      "FILE: ./data/endata_new_clean/camera-compsource-university-collegeboard/clean_0000.htm\n",
      "\n",
      "1 - 1 - 0\n",
      "PATH: ./data/endata/camera/camera-ecost(923)/0000.htm\n",
      "FILE: ./data/endata_new_clean/camera-ecost-university-collegeboard/clean_0000.htm\n",
      "\n",
      "1 - 1 - 0\n",
      "PATH: ./data/endata/camera/camera-jr(367)/0000.htm\n",
      "FILE: ./data/endata_new_clean/camera-jr-university-collegeboard/clean_0000.htm\n",
      "\n",
      "1 - 1 - 0\n",
      "PATH: ./data/endata/camera/camera-newegg(220)/0000.htm\n",
      "FILE: ./data/endata_new_clean/camera-newegg-university-collegeboard/clean_0000.htm\n",
      "\n",
      "1 - 1 - 0\n",
      "PATH: ./data/endata/camera/camera-onsale(261)/0000.htm\n",
      "FILE: ./data/endata_new_clean/camera-onsale-university-collegeboard/clean_0000.htm\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 129214: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ardiz\\OneDrive\\Desktop\\Projects\\Webformer\\Preprocess\\html2json.ipynb Cell 10\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ardiz/OneDrive/Desktop/Projects/Webformer/Preprocess/html2json.ipynb#X13sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFILE: \u001b[39m\u001b[39m{\u001b[39;00mclean_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ardiz/OneDrive/Desktop/Projects/Webformer/Preprocess/html2json.ipynb#X13sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m cleaner \u001b[39m=\u001b[39m HTMLCleaner(path,clean_path)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ardiz/OneDrive/Desktop/Projects/Webformer/Preprocess/html2json.ipynb#X13sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m cleaner\u001b[39m.\u001b[39;49mclean_html()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ardiz/OneDrive/Desktop/Projects/Webformer/Preprocess/html2json.ipynb#X13sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m#cleaner.clean_tree()\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ardiz/OneDrive/Desktop/Projects/Webformer/Preprocess/html2json.ipynb#X13sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m#storer = HTMLStorer(input_file=None,html=cleaner.line)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ardiz/OneDrive/Desktop/Projects/Webformer/Preprocess/html2json.ipynb#X13sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\ardiz\\OneDrive\\Desktop\\Projects\\Webformer\\Preprocess\\html2json.ipynb Cell 10\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ardiz/OneDrive/Desktop/Projects/Webformer/Preprocess/html2json.ipynb#X13sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m style \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ardiz/OneDrive/Desktop/Projects/Webformer/Preprocess/html2json.ipynb#X13sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_file, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ardiz/OneDrive/Desktop/Projects/Webformer/Preprocess/html2json.ipynb#X13sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     line \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mread()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ardiz/OneDrive/Desktop/Projects/Webformer/Preprocess/html2json.ipynb#X13sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     annotation\u001b[39m.\u001b[39mextend(annotation_pattern\u001b[39m.\u001b[39mfindall(line))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ardiz/OneDrive/Desktop/Projects/Webformer/Preprocess/html2json.ipynb#X13sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     \u001b[39m#print(annotation)\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2032.0_x64__qbz5n2kfra8p0\\Lib\\encodings\\cp1252.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[39mreturn\u001b[39;00m codecs\u001b[39m.\u001b[39mcharmap_decode(\u001b[39minput\u001b[39m,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39merrors,decoding_table)[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9d in position 129214: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "\n",
    "# Open Train Corpus file\n",
    "with open(JSON_TRAIN_CORPUS,'w')as g:\n",
    "    for root, dirs, files in os.walk(DIR_RAW_DATA_PATH):\n",
    "\n",
    "        iter_i = 0\n",
    "        # Iter ZERO\n",
    "        for dir in dirs:\n",
    "            #print(\"DIR: \"+dir)\n",
    "            sub_DIR_RAW_DATA_PATH = os.path.join(root,dir)\n",
    "            # Get folder\n",
    "            iter_j=0\n",
    "            # Iter ONE\n",
    "            for sub_root,sub_dirs,sub_files in os.walk(sub_DIR_RAW_DATA_PATH):\n",
    "                if iter_j > 0:\n",
    "                    extracted_name = sub_root.split(\"\\\\\")[1].split(\"(\")[0]\n",
    "                    #print(\"FOLDER: \"+extracted_name)\n",
    "                iter_j += 1\n",
    "                # Get file\n",
    "                iter_k = 0\n",
    "                # Iter TWO\n",
    "                for item_parsed in sub_files:\n",
    "                    path = os.path.join(sub_root,item_parsed)\n",
    "                    path = re.sub(r'\\\\', '/', path)\n",
    "                    clean_path = os.path.join(DIR_CLEAN_DATA,dir+extracted_name+\"/clean_\"+item_parsed)\n",
    "                    clean_path = re.sub(r'\\((\\d+)\\)', '-', clean_path)\n",
    "                    print(f\"{iter_i} - {iter_j} - {iter_k}\")\n",
    "                    print(f\"PATH: {path}\")\n",
    "                    print(f\"FILE: {clean_path}\")\n",
    "                    cleaner = HTMLCleaner(path,clean_path)\n",
    "                    cleaner.clean_html()\n",
    "                    #cleaner.clean_tree()\n",
    "                    #storer = HTMLStorer(input_file=None,html=cleaner.line)\n",
    "                    print(\"\")\n",
    "                    if iter_k == 0:\n",
    "                        break\n",
    "                    iter_k += 1\n",
    "\n",
    "                if iter_j == 2:\n",
    "                    break\n",
    "            iter_i =+1 \n",
    "\n",
    "                \n",
    "\n",
    "        \"\"\" \n",
    "        for dir in tqdm(dirs):\n",
    "            sub_DIR_RAW_DATA_PATH = os.path.join(root,dir)\n",
    "            for sub_root,sub_dirs,sub_files in os.walk(sub_DIR_RAW_DATA_PATH):\n",
    "                # foeach file in dir\n",
    "                for item_parsed in tqdm(sub_files):\n",
    "                    #index = item_parsed.split('.')[0]\n",
    "                    path = os.path.join(sub_root,item_parsed)\n",
    "                    clean_path = DIR_CLEAN_DATA+'_clean.html'\n",
    "                    cleaner = HTMLCleaner(path,clean_path)\n",
    "                    cleaner.clean_html()\n",
    "                    #cleaner.clean_tree()\n",
    "                    storer = HTMLStorer(input_file=None,html=cleaner.line)\n",
    "                    g.write(json.dumps(storer.data,ensure_ascii=False)+'\\n') \"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
